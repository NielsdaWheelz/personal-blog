---
title: 'Zero to Hero - One'
publishedAt: '2025-11-18'
summary: ''
---

a network is a group of people trying to guess the next note based on the the last few notes. each has their own opinions, tries to guess what comes next, updates microscopically when they're wrong, and slowly the whole group becomes good at predicting notes.

they start out terrible. they guess randomly. but every time they're wrong, they tweak themselves a little. after repeating this thousands of times, they become surprisingly good at it.

1. embeddings

first you have to turn the notes into something the people can understand. unfortunately they're autistic and don't understand notes or sounds, so you just turn them into numbers. think of it like giving each note a little personality profile: a bunch of random numbers between -1 and 1. 

a -> [0.1, -0.3, 0.2, …]
b -> [0.7, -0.1, 0.0, …]
c -> [-0.2, 0.4, 0.8, …]

this is an embedding.

these numbers are coordinates (like x and y and z and ...). they have positions in space. as we train the model, the positions of the notes will move (the coordinates will change) according to what notes appear in similar spots.

these numbers and dimensions have no meaning. there can be 1 or 1 gajillion. they are not traits. they are not anything. they are knobs for the machine to tweak.

we'll use 2-dimensions for now (like x and y)

2. context

the network of guessers looks at the input (last three notes) to guess the next note. remember, each of these notes isn't actually a note, but a series of numbers (aka a vector). we'll squash these 3 vectors into one big vector; 3 * 2-dimensions = 6-dimensional vector. literally just concatenation. this is the context the people receive.

3. the hidden layer

this big vector gets thrown to the room; each of the 10 people in the room looks at the same 6 numbers, but each one hears a little differently. they all have their own private mix of the 6 numbers.

each guesser has a list of 'hearing strengths' (aka weights)
they multiply each input number (each coordinate) by their own strength, sum it up, and add a personal bias.

we squash the result through a function which only returns values between -1 and 1 (e.g. `tanh`). this is to make the network nonlinear. if we didn't, the end result would just be a line; now it can be composed of infinite tiny curves.

this isn't a 'guess' of what the next note is. they build an intermediate representation. they just transform the input. this isn't a guess about what the next token is, these are arbitrary numbers that makes prediction easier downstream.

the output of these 10 people is 10 new numbers: the hidden layer. these numbers are also completely nameless and meaningless.

4. the final guesser

now all 10 hidden outputs are passed to one final, slightly fussy guesser who:
- looks at those 10 numbers
- scores every possible next note
- outputs a list of 7 scores (for 7 notes: a b c d e f g) - the 'logits'

we turn these into probabilities (via a function, softmax):
a: 0.45
b: 0.22
c: 0.01
…

then, if the real next note was a, but the network leaned toward b, we record the difference between the _true_ next note and the _predicted_ next note: the loss.

5. backprop

each parameter (weights, biases, embeddings) gets a gradient (a slope of how the loss changes if that parameter changes). then we nudge it by `parameter <- parameter - learning_rate * gradient`

this is the learning. the error flows backward, and everyone in the room gets blamed in proportion to how badly they contributed to the fucked prediction. the final guesser, the hidden guessers, and the embeddings (the coordinates) all adjust.

do this 200,000 times, and the room becomes _really_ good at guessing the next note.

6. generating songs

to generate a song, you just let the room guess over and over:
- start with a rest ('.')
- guess the next note -> 'e'
- feed 'e' back in
- guess again -> 'f'
- guess again -> 'g'
- guess again -> 'a'
- it predicts a rest ('.') again

in one sentence: three notes go in, get turned into coordinates, mixed around by a bunch of weights and biases, scored against possible next notes, and the whole system nudges its numbers slightly depending on how wrong it was; then repeat.